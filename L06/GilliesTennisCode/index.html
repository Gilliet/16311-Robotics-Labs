<!DOCTYPE  html>
<!--url of page: www.andrew.cmu.edu/usr/gtr/16311/lab01/index.html-->
<html>
    <head>
        <style type="text/css">
            h1 {font-family:Helvetica;}
            p {font-family:Helvetica;}
            ol {font-family:Helvetica;}
            li {font-family:Helvetica;}
            table {font-family:Helvetica;}
            tr {font-family:Helvetica;}
            td {font-family:Helvetica;}
        </style>
    </head>
    <body>
    <h1>Lab 02: Vision Algorithms </h1>
    <p>
    Gillian Rosen / gtr / 16-311: Intro to Robotics
    </p>
    <hr>
    <p> 
    Meet my image pipeline! 
    <br>
    <br>
   <img src="intermed/centsB16.png" alt="flavor1" width="200" height="150"> 
   <img src="intermed/centsB08.png" alt="flavor1" width="200" height="150">
   <img src="intermed/centsB04.png" alt="flavor1" width="200" height="150">
   <img src="intermed/centsB02.png" alt="flavor1" width="200" height="150">
   <br>
    <br>
    <br>
    <a href="http://www.andrew.cmu.edu/user/gtr/16311/lab02/bigImgs.html">
        Photo progressions for big target</a>
    <br>

    <a href="http://www.andrew.cmu.edu/user/gtr/16311/lab02/littleImgs.html">
        Photo progressions for little target</a>
    <br>
    <br>
    <!--
    Energy transfers: 
    <ol>
        <li>pulley carries golf ball up tower</li>
    </ol>
    -->
    <br>
    <p>
    <b>Data Collection Methods</b>
    <br>
    I used my phone, a four-year-old Samsung Strive SGH-A687, to take 
    pictures of the two targets.
    For the small (5")target, my phone was positioned with its top 34" up
    the meterstick. It was positioned in "portrait" orientation. For the
    large (15") target, the top of the phone was 9" up the meterstick, and
    the phone was positioned in "landscape" orientation.
    <br>
    I counted the 1' floor tiles to position myself accurately with respect 
    to the targets, then verified my distance with a tape 
    measure. I then placed the meterstick at the edge of the appropriate 
    floor tile and positioned my phone at either 9" or 34" up the stick, as
    specified above. I took three pictures at each distance and kept the 
    best of the them for processing. My photos were taken at 11:00 AM on 
    Friday, 1/24.   
    </p>
    <br>
    <p>
    <b>Estimation Errors (in feet)</b>
    <br>
    <!-- table goes here-->
    <table border="1">
        <tr>
            <td> actual distance </td>
            <td> Little Target </td>
            <td> Big Target </td>
        </tr>
        <tr>
            <td>2</td>
            <td>-0.00093</td>
            <td>-0.0005</td>
        </tr>
        <tr>
            <td>4</td>
            <td>-0.02788</td>
            <td>-0.02296</td>
        </tr> 
        <tr>
            <td>8</td>
            <td>0.104435</td>
            <td>0.080972</td>
        </tr>
        <tr>
            <td>16</td>
            <td>-0.13324</td>
            <td>-0.08912</td>
        </tr>
    </table>
    <br>
   <img src="trendlineErrors.PNG" alt="trendline errors" width="788" height="453"> 
    <br>
    </p>
    <br>
    <p>
    <b>Analysis</b>
    <br>
    The error in my estimates is fairly small, but seems to increase in
    magnitude when the target is far away. The error for the little target
    is consistently larger in magnitude than the error for the big target.<br>
    One possible reason for this is that the little target has brown 
    cardboard boxes behind it, as well as more yellow-toned background 
    elements in general. My initial thresholding had to be particularly 
    aggressive to combat this, so a lot of the small target's 
    tennis ball area was lost. <br>
    Another factor that definitely influenced 
    error was the quality of my phone's camera. It is fairly old and its
    photos are prone to graininess and poor color quality.  
    <br>
    It is also possible that the automatic color adjustments made by my 
    phone could be problematic. The adjustments that it made on the tennis
   balls' color hue to the brown
   background of the boxes made their hue too different from the hue of 
   the tennis balls on the white background. <br><br>
   To test this, I placed a piece of white paper behind the small target,
   took another picture at 4 feet and at 8 feet, and ran it through the pipeline. 
   <br>
   <img src="intermed/threshrs4.png" alt="flavor1" width="150" height="200"> 
   <img src="intermed/cleanrs4.png" alt="flavor1" width="150" height="200">
   <img src="intermed/segmentedrs4.png" alt="flavor1" width="150" height="200">
   <img src="intermed/centsrs4.png" alt="flavor1" width="150" height="200"> 
   <br>
   <img src="intermed/threshrs8.png" alt="flavor1" width="150" height="200"> 
   <img src="intermed/cleanrs8.png" alt="flavor1" width="150" height="200">
   <img src="intermed/segmentedrs8.png" alt="flavor1" width="150" height="200">
   <img src="intermed/centsrs8.png" alt="flavor1" width="150" height="200"> 
  
   <br>
   
   The images turned out to actually be too good, calculating 4 feet as 10.7 feet and 8 feet as 14 feet. Due to the clearly-defined
   tennis ball images, the heuristics that I had tuned on poor-quality 
   images, particularly the area-based heuristic, overestimated the distance
   to the target. In the future, I should work to make my heuristic more 
   robust to changes in color. 
   
   </p>
    <p>
    <br>
    <br>
    <br>
    16-311: Intro to Robotics at Carnegie Mellon University
    <br> Spring 2014
    </p>
</body>
</html>
